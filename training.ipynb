{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e974a703-b27d-4b50-a4b3-484287a00c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fc99cd-254c-4570-9328-fa39494d0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_path = \"assets/transformed data/train_data_cat.pkl\"\n",
    "cat_test_path = \"assets/transformed data/test_data_cat.pkl\"\n",
    "dog_train_path = \"assets/transformed data/train_data_dog.pkl\"\n",
    "dog_test_path = \"assets/transformed data/test_data_dog.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190f9940-8963-423c-90d8-92610be46d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- DATASET ---------\n",
    "class SiamesePairDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, train_bool: bool):\n",
    "\n",
    "        with open(dataset_dir, \"rb\") as f:\n",
    "            self.dataset = pickle.load(f)\n",
    "        self.train_bool = train_bool\n",
    "        self.horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "        self.user_imgs = [item for item in self.dataset if item[1] == 1]\n",
    "        self.neg_imgs = [item for item in self.dataset if item[1] == 0]\n",
    "        self.all_pairs = []\n",
    "\n",
    "        for i in range(len(self.user_imgs)):\n",
    "            for j in range(i+1, len(self.user_imgs)):\n",
    "                self.all_pairs.append((self.user_imgs[i][0], self.user_imgs[j][0], 1))\n",
    "\n",
    "        for i in range(min(len(self.user_imgs), len(self.neg_imgs))):\n",
    "            self.all_pairs.append((self.user_imgs[i][0], self.neg_imgs[i][0], 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p1, p2, label = self.all_pairs[idx]\n",
    "\n",
    "        if self.train_bool:\n",
    "            return self.horizontal_flip(p1), self.horizontal_flip(p2), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        else:\n",
    "            return p1, p2, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638f2504-e165-485c-b742-998971410f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- MODEL ---------\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.features = mobilenet.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fe3913-fda9-4688-b402-63cad8c6abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- CONTRASTIVE LOSS ---------\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, o1, o2, label):\n",
    "        dist = torch.nn.functional.pairwise_distance(o1, o2)\n",
    "        loss = label * dist**2 + (1 - label) * torch.clamp(self.margin - dist, min=0)**2\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec00e670-52f0-46ad-a1d5-040e69af19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- TRAINING ---------\n",
    "def train_siamese(model, dataloader, epochs=10):\n",
    "    model.to(DEVICE)\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # variables to stop the training if loss doesn't improve\n",
    "    best_loss = float('inf')\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        for x1, x2, y in pbar:\n",
    "            x1, x2, y = x1.to(DEVICE), x2.to(DEVICE), y.to(DEVICE)\n",
    "            out1, out2 = model(x1), model(x2)\n",
    "            loss = criterion(out1, out2, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        end_epoch = time.time()\n",
    "        print(f\"Epoch {epoch+1} Summary: Avg Loss = {total_loss / len(dataloader):.4f} | Time: {end_epoch - start_epoch:.2f}s\")\n",
    "\n",
    "\n",
    "        # Below stops training if loss doesn't improve\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), f\"assets/trained models/model_temp_epoch_{epoch+1}.pt\")\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d19aaff9-1ec6-4f08-89f5-bd34ae3000fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train dogs, pass dog_train_path to the dataset\n",
    "# To train cats, pass cat_train_path to the dataset\n",
    "dataset = SiamesePairDataset(cat_train_path, train_bool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02475548-6810-4dcc-919a-5888b94f0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01bb5640-3f3f-4557-9aed-e873cd7da215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\socam\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\socam\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = EmbeddingNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f883d26d-3a70-4d9d-b43c-b2ea523cc1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  5.96it/s, loss=0.0223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary: Avg Loss = 0.1721 | Time: 7.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  6.26it/s, loss=0.0181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary: Avg Loss = 0.1622 | Time: 7.21s\n"
     ]
    }
   ],
   "source": [
    "train_siamese(model, loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766ba5f-51bb-42a4-a2e5-9070278399e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d0c282a-cc63-46a9-bec8-eb9b9540a57a",
   "metadata": {},
   "source": [
    "Below code allows to see what happens when Horizontal flip happens. Change value p value in  RandomHorizontalFlip(p=1)\n",
    "\n",
    "to visually siee the difference between flipped and not flipped images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b96bbd7c-11d5-4352-a7a9-0455cd50355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"assets/transformed data/train_data_dog.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Define the transform\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=0)\n",
    "\n",
    "# see the image\n",
    "tens = data[1][0]\n",
    "\n",
    "# make the flip\n",
    "tens = horizontal_flip(tens)\n",
    "\n",
    "# Undo normalization to see the picture\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=tens.device).view(-1, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device=tens.device).view(-1, 1, 1)\n",
    "\n",
    "b = (tens * std ) + mean\n",
    "\n",
    "# Convert to PIL image\n",
    "to_pil = transforms.ToPILImage()\n",
    "image = to_pil(b)\n",
    "\n",
    "# Show the image\n",
    "image.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
