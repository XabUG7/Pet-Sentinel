{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564ce905-01b1-4c08-a897-fe15d70d02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec2004e-c3ad-448d-82ff-d40fe4e19ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# transform from tensor in the form of np.array to normalised tensor\n",
    "def tensor_transform(tensor_as_np_array):\n",
    "    # Resize to (224, 224)\n",
    "    tensor = torch.from_numpy(tensor_as_np_array).float()\n",
    "    tensor = nn.functional.interpolate(tensor.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "    \n",
    "    # Normalize using mean and std\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(-1, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(-1, 1, 1)\n",
    "    tensor = (tensor - mean) / std\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad494d8-93bb-4475-9b32-8fe9004900b7",
   "metadata": {},
   "source": [
    "#### Get the full images from h5 files and resize them to be uniform size and normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b5ff1c-172c-4959-b15a-12221de8442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 546.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 404.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 424.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 509.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# The files that have the animals images are the following\n",
    "h5_files_cat = [\"assets/raw dataset/pet_sentinel_data_random cats.h5\", \"assets/raw dataset/pet_sentinel_data_same cat.h5\"]\n",
    "h5_files_dog = [\"assets/raw dataset/pet_sentinel_data_random dogs.h5\", \"assets/raw dataset/pet_sentinel_data_same dog.h5\"]\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over the each animal files\n",
    "for h5_files in [h5_files_cat, h5_files_dog]:\n",
    "\n",
    "    # This list will be saved as a pickle file\n",
    "    tensor_label_list = []\n",
    "\n",
    "    # Iterate over same animal files\n",
    "    # first file is random so label is 0\n",
    "    # second file is same so label is 1\n",
    "    for label in range(len(h5_files)):\n",
    "\n",
    "        # Get animal to include it in pickle file name\n",
    "        if \"cat\" in h5_files[label]:\n",
    "            animal = \"cat\"\n",
    "        elif \"dog\" in h5_files[label]:\n",
    "            animal = \"dog\"\n",
    "        \n",
    "        tensor_dict = {}\n",
    "        label_dict = {}\n",
    "\n",
    "        # Open h5 file and save data to tensor_dict\n",
    "        with h5py.File(h5_files[label], 'r') as f:\n",
    "            for key in f:\n",
    "                group = f[key]\n",
    "                tensors = []\n",
    "                labels = []\n",
    "                for dname in group:\n",
    "                    dset = group[dname]\n",
    "                    tensors.append(dset[()])\n",
    "                    labels.append(dset.attrs['label'])\n",
    "                tensor_dict[key] = tensors\n",
    "                label_dict[key] = labels\n",
    "        \n",
    "        for tensor_as_np in tqdm(list(tensor_dict.values())[0]):\n",
    "            normalized_tensor = tensor_transform(tensor_as_np)\n",
    "            tensor_label_list.append((normalized_tensor, label))\n",
    "\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(f\"assets/transformed data/tensor_label_data_{animal}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tensor_label_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72509c9-3848-4704-b99d-3fc065925838",
   "metadata": {},
   "source": [
    "## Take the transformed data from the pickle files and create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ffec40-22b0-498d-b504-8bd9d51cc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test datasets\n",
    "for animal in [\"cat\", \"dog\"]:\n",
    "\n",
    "    # Load data for each animal\n",
    "    with open(f\"assets/transformed data/tensor_label_data_{animal}.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Separate by label\n",
    "    data_0 = [item for item in data if item[1] == 0]  # other pets\n",
    "    data_1 = [item for item in data if item[1] == 1]  # your pet\n",
    "    \n",
    "    # Balance training set\n",
    "    train_ratio = 0.9\n",
    "    n_train_1 = int(len(data_1) * train_ratio)\n",
    "    n_train_0 = n_train_1\n",
    "\n",
    "    train_data_1 = random.sample(data_1, n_train_1)\n",
    "    train_data_0 = random.sample(data_0, n_train_0)\n",
    "    train_data = train_data_0 + train_data_1\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    # Save to pickle file\n",
    "    with open(f\"assets/transformed data/train_data_{animal}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    \n",
    "    # Remaining data goes to test set (imbalanced)\n",
    "    train_ids_1 = set(id(t[0]) for t in train_data_1)\n",
    "    test_data_1 = [item for item in data_1 if id(item[0]) not in train_ids_1]\n",
    "    \n",
    "    train_ids_0 = set(id(t[0]) for t in train_data_0)\n",
    "    test_data_0 = [item for item in data_0 if id(item[0]) not in train_ids_0]\n",
    "    test_data = test_data_0 + test_data_1\n",
    "    random.shuffle(test_data)\n",
    "    \n",
    "    with open(f\"assets/transformed data/test_data_{animal}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1adf4-38fd-40dd-97ee-8eb1cf32c750",
   "metadata": {},
   "source": [
    "#### Below code allows to see the images after resizing and before normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05a0a018-6575-4e5c-b279-7d2e4c122074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open(f\"assets/transformed data/test_data_cat.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# see the image\n",
    "tens = data[0][0]\n",
    "\n",
    "# Normalize using ImageNet mean and std\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=tens.device).view(-1, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device=tens.device).view(-1, 1, 1)\n",
    "\n",
    "b = (tens * std ) + mean\n",
    "\n",
    "# Convert to PIL image\n",
    "to_pil = transforms.ToPILImage()\n",
    "image = to_pil(b)\n",
    "\n",
    "# Show the image\n",
    "image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
