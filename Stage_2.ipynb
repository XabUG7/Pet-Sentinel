{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pillow-heif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tItPfFqLzTRW",
        "outputId": "63729604-9453-472b-99df-22b94a75c04d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow-heif\n",
            "  Downloading pillow_heif-1.1.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow-heif) (11.3.0)\n",
            "Downloading pillow_heif-1.1.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/5.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow-heif\n",
            "Successfully installed pillow-heif-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "6a_KZDjioH8K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import uuid\n",
        "from uuid import uuid4\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from pillow_heif import register_heif_opener, open_heif\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otnKFOEUC-o-",
        "outputId": "a7152f6a-f5bd-4e89-be42-337d965a6252"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "register_heif_opener()"
      ],
      "metadata": {
        "id": "1bic1vYWzaN_"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- CONFIG ---------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR = Path(\"pet_verification\")\n",
        "USER_DIR = BASE_DIR / \"user_pet\"\n",
        "NEG_DIR = BASE_DIR / \"test_dataset\"\n",
        "GALLERY_PATH = BASE_DIR / \"my_pet_gallery.pkl\"\n",
        "PAIRS_CACHE = BASE_DIR / \"pair_cache.pkl\""
      ],
      "metadata": {
        "id": "5EPBGLTSoUzH"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- UTILITY: Convert HEIC to JPG ---------\n",
        "def convert_heic_to_jpg(directory):\n",
        "    for path in Path(directory).rglob(\"*\"):\n",
        "        if path.suffix.lower() == \".heic\":\n",
        "            try:\n",
        "                image = Image.open(path).convert(\"RGB\")\n",
        "                jpg_path = path.with_suffix(\".jpg\")\n",
        "                image.save(jpg_path)\n",
        "                print(f\"Converted {path.name} → {jpg_path.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to convert {path.name}: {e}\")"
      ],
      "metadata": {
        "id": "AlQn28jYGgfU"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- TRANSFORMS ---------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "Yie6ZgVE3Q3R"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_from_videos(base_dir, exts=('.mov', '.mp4', '.avi')):\n",
        "    for folder in [USER_DIR, NEG_DIR]:\n",
        "        for file in Path(folder).glob(\"**/*\"):\n",
        "            if file.suffix.lower() in exts:\n",
        "                cap = cv2.VideoCapture(str(file))\n",
        "                fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "                interval = int(fps * 0.5) if fps > 0 else 1\n",
        "                frame_id, save_id = 0, 0\n",
        "                base_name = file.stem\n",
        "                while cap.isOpened():\n",
        "                    ret, frame = cap.read()\n",
        "                    if not ret:\n",
        "                        break\n",
        "                    if frame_id % max(1, interval) == 0:\n",
        "                        out_path = file.parent / f\"{base_name}_frame{save_id:03d}.jpg\"\n",
        "                        cv2.imwrite(str(out_path), frame)\n",
        "                        save_id += 1\n",
        "                    frame_id += 1\n",
        "                cap.release()\n",
        "                print(f\"Extracted {save_id} frames from {file.name}\")\n",
        "                # Ignore video file after extraction by renaming it\n",
        "                ignored_path = file.with_suffix(file.suffix + \".ignore\")\n",
        "                file.rename(ignored_path)\n"
      ],
      "metadata": {
        "id": "mFwuLJF05ZHR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- DATASET ---------\n",
        "class SiamesePairDataset(Dataset):\n",
        "    def __init__(self, user_dir, neg_dir, transform):\n",
        "        self.transform = transform\n",
        "        self.user_imgs = sorted([p for p in Path(user_dir).glob(\"**/*\") if p.suffix.lower() in ['.jpg', '.jpeg', '.png'] and not p.name.endswith(\".ignore\")])[:50]\n",
        "        self.neg_imgs = sorted([p for p in Path(neg_dir).glob(\"**/*\") if p.suffix.lower() in ['.jpg', '.jpeg', '.png'] and not p.name.endswith(\".ignore\")])[:50]\n",
        "        self.all_pairs = []\n",
        "\n",
        "        for i in range(len(self.user_imgs)):\n",
        "            for j in range(i+1, len(self.user_imgs)):\n",
        "                self.all_pairs.append((self.user_imgs[i], self.user_imgs[j], 1))\n",
        "\n",
        "        for i in range(min(len(self.user_imgs), len(self.neg_imgs))):\n",
        "            self.all_pairs.append((self.user_imgs[i], self.neg_imgs[i], 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p1, p2, label = self.all_pairs[idx]\n",
        "        try:\n",
        "            img1 = Image.open(p1).convert(\"RGB\")\n",
        "            img2 = Image.open(p2).convert(\"RGB\")\n",
        "        except (UnidentifiedImageError, OSError):\n",
        "            raise RuntimeError(f\"Failed to load: {p1} or {p2}\")\n",
        "        return self.transform(img1), self.transform(img2), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y5QhNEjD3T9H"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- MODEL ---------\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "        self.features = mobilenet.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1280, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZOtv07HG3VlQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- CONTRASTIVE LOSS ---------\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, o1, o2, label):\n",
        "        dist = torch.nn.functional.pairwise_distance(o1, o2)\n",
        "        loss = label * dist**2 + (1 - label) * torch.clamp(self.margin - dist, min=0)**2\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "2glSsOEq3XO2"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- TRAINING ---------\n",
        "def train_siamese(model, dataloader, epochs=10):\n",
        "    model.to(DEVICE)\n",
        "    criterion = ContrastiveLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        start_epoch = time.time()\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "        for x1, x2, y in pbar:\n",
        "            x1, x2, y = x1.to(DEVICE), x2.to(DEVICE), y.to(DEVICE)\n",
        "            out1, out2 = model(x1), model(x2)\n",
        "            loss = criterion(out1, out2, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        end_epoch = time.time()\n",
        "        print(f\"Epoch {epoch+1} Summary: Avg Loss = {total_loss / len(dataloader):.4f} | Time: {end_epoch - start_epoch:.2f}s\")\n",
        "        torch.save(model.state_dict(), BASE_DIR / f\"model_epoch_{epoch+1}.pt\")\n"
      ],
      "metadata": {
        "id": "zxlNOS8s3YEj"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- EMBEDDING GENERATION ---------\n",
        "def save_gallery_embeddings(model):\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "    embeddings = []\n",
        "    for img_path in USER_DIR.glob(\"**/*\"):\n",
        "        if img_path.suffix.lower() not in ['.jpg', '.jpeg', '.png']:\n",
        "            continue\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            img_tensor = base_transform(img).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                emb = model(img_tensor).cpu().numpy().flatten()\n",
        "                embeddings.append(emb)\n",
        "        except:\n",
        "            continue\n",
        "    with open(GALLERY_PATH, \"wb\") as f:\n",
        "        pickle.dump(np.array(embeddings), f)\n",
        "    print(\"Gallery embeddings saved.\")"
      ],
      "metadata": {
        "id": "rM1ly6Ul3ZeR"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- VERIFICATION ---------\n",
        "def verify_folder(model, folder_path, gallery_path=GALLERY_PATH, threshold=0.75):\n",
        "    with open(gallery_path, \"rb\") as f:\n",
        "        gallery = pickle.load(f)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "    files = [f for f in Path(folder_path).glob(\"**/*\") if f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
        "\n",
        "    match_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            img = Image.open(file).convert(\"RGB\")\n",
        "            tensor = base_transform(img).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                emb = model(tensor).cpu().numpy().flatten()\n",
        "            score = np.mean(cosine_similarity([emb], gallery))\n",
        "            is_match = score >= threshold\n",
        "            result = \"MATCH\" if is_match else \"NOT YOUR PET\"\n",
        "            print(f\"{file.name}: Similarity = {score:.4f} → {result}\")\n",
        "            total_count += 1\n",
        "            if is_match:\n",
        "                match_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to verify {file.name}: {e}\")\n",
        "\n",
        "    not_pet_rate = (total_count - match_count) / total_count if total_count else 0\n",
        "    print(f\"\\n Total files: {total_count}\")\n",
        "    print(f\"Matches: {match_count}\")\n",
        "    print(f\"Not Your Pet: {total_count - match_count}\")\n",
        "    print(f\"Not Your Pet Rate: {not_pet_rate:.2%}\")"
      ],
      "metadata": {
        "id": "BLhPk6YY3bLq"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_heic_to_jpg(BASE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vinakVMqGpN3",
        "outputId": "e6fc9f2c-1807-47f3-8bfe-390bf12bb71a"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted IMG_9721.HEIC → IMG_9721.jpg\n",
            "Converted IMG_9718.HEIC → IMG_9718.jpg\n",
            "Converted IMG_9688.HEIC → IMG_9688.jpg\n",
            "Converted IMG_9681.HEIC → IMG_9681.jpg\n",
            "Converted IMG_9808.HEIC → IMG_9808.jpg\n",
            "Converted IMG_9743.HEIC → IMG_9743.jpg\n",
            "Converted IMG_9675.HEIC → IMG_9675.jpg\n",
            "Converted IMG_9673.HEIC → IMG_9673.jpg\n",
            "Converted IMG_9687.HEIC → IMG_9687.jpg\n",
            "Converted IMG_9706.HEIC → IMG_9706.jpg\n",
            "Converted IMG_9661.HEIC → IMG_9661.jpg\n",
            "Converted IMG_9800.HEIC → IMG_9800.jpg\n",
            "Converted IMG_9709.HEIC → IMG_9709.jpg\n",
            "Converted IMG_9660.HEIC → IMG_9660.jpg\n",
            "Converted IMG_9802.HEIC → IMG_9802.jpg\n",
            "Converted IMG_9754.HEIC → IMG_9754.jpg\n",
            "Converted IMG_9680.HEIC → IMG_9680.jpg\n",
            "Converted IMG_9702.HEIC → IMG_9702.jpg\n",
            "Converted IMG_9809.HEIC → IMG_9809.jpg\n",
            "Converted IMG_9667.HEIC → IMG_9667.jpg\n",
            "Converted IMG_9670.HEIC → IMG_9670.jpg\n",
            "Converted IMG_9810.HEIC → IMG_9810.jpg\n",
            "Converted IMG_9677.HEIC → IMG_9677.jpg\n",
            "Converted IMG_9657.HEIC → IMG_9657.jpg\n",
            "Converted IMG_9745.HEIC → IMG_9745.jpg\n",
            "Converted IMG_9749.HEIC → IMG_9749.jpg\n",
            "Converted IMG_9731.HEIC → IMG_9731.jpg\n",
            "Converted IMG_9796.HEIC → IMG_9796.jpg\n",
            "Converted IMG_9676.HEIC → IMG_9676.jpg\n",
            "Converted IMG_9785.HEIC → IMG_9785.jpg\n",
            "Converted IMG_9748.HEIC → IMG_9748.jpg\n",
            "Converted IMG_9682.HEIC → IMG_9682.jpg\n",
            "Converted IMG_9752.HEIC → IMG_9752.jpg\n",
            "Converted IMG_9787(1).HEIC → IMG_9787(1).jpg\n",
            "Converted IMG_9735.HEIC → IMG_9735.jpg\n",
            "Converted IMG_9807.HEIC → IMG_9807.jpg\n",
            "Converted IMG_9697.HEIC → IMG_9697.jpg\n",
            "Converted IMG_9797.HEIC → IMG_9797.jpg\n",
            "Converted IMG_9662.HEIC → IMG_9662.jpg\n",
            "Converted IMG_9764.HEIC → IMG_9764.jpg\n",
            "Converted IMG_9804.HEIC → IMG_9804.jpg\n",
            "Converted IMG_9781.HEIC → IMG_9781.jpg\n",
            "Converted IMG_9727.HEIC → IMG_9727.jpg\n",
            "Converted IMG_9694.HEIC → IMG_9694.jpg\n",
            "Converted IMG_9734.HEIC → IMG_9734.jpg\n",
            "Converted IMG_9739.HEIC → IMG_9739.jpg\n",
            "Converted IMG_9742.HEIC → IMG_9742.jpg\n",
            "Converted IMG_9695.HEIC → IMG_9695.jpg\n",
            "Converted IMG_9787.HEIC → IMG_9787.jpg\n",
            "Converted IMG_9777.HEIC → IMG_9777.jpg\n",
            "Converted IMG_9784.HEIC → IMG_9784.jpg\n",
            "Converted IMG_9711.HEIC → IMG_9711.jpg\n",
            "Converted IMG_9798.HEIC → IMG_9798.jpg\n",
            "Converted IMG_9722.HEIC → IMG_9722.jpg\n",
            "Converted IMG_9753.HEIC → IMG_9753.jpg\n",
            "Converted IMG_9733.HEIC → IMG_9733.jpg\n",
            "Converted IMG_9699.HEIC → IMG_9699.jpg\n",
            "Converted IMG_9693.HEIC → IMG_9693.jpg\n",
            "Converted IMG_9669.HEIC → IMG_9669.jpg\n",
            "Converted IMG_9708.HEIC → IMG_9708.jpg\n",
            "Converted IMG_9656.HEIC → IMG_9656.jpg\n",
            "Converted IMG_9678.HEIC → IMG_9678.jpg\n",
            "Converted IMG_9773.HEIC → IMG_9773.jpg\n",
            "Converted IMG_9786.HEIC → IMG_9786.jpg\n",
            "Converted IMG_9707.HEIC → IMG_9707.jpg\n",
            "Converted IMG_9716.HEIC → IMG_9716.jpg\n",
            "Converted IMG_9779.HEIC → IMG_9779.jpg\n",
            "Converted IMG_9692.HEIC → IMG_9692.jpg\n",
            "Converted IMG_9712.HEIC → IMG_9712.jpg\n",
            "Converted IMG_9741.HEIC → IMG_9741.jpg\n",
            "Converted IMG_9685.HEIC → IMG_9685.jpg\n",
            "Converted IMG_9803.HEIC → IMG_9803.jpg\n",
            "Converted IMG_9683.HEIC → IMG_9683.jpg\n",
            "Converted IMG_9659.HEIC → IMG_9659.jpg\n",
            "Converted IMG_9698.HEIC → IMG_9698.jpg\n",
            "Converted IMG_9720.HEIC → IMG_9720.jpg\n",
            "Converted IMG_9704.HEIC → IMG_9704.jpg\n",
            "Converted IMG_9757.HEIC → IMG_9757.jpg\n",
            "Converted IMG_9732.HEIC → IMG_9732.jpg\n",
            "Converted IMG_9759.HEIC → IMG_9759.jpg\n",
            "Converted IMG_9751.HEIC → IMG_9751.jpg\n",
            "Converted IMG_9737.HEIC → IMG_9737.jpg\n",
            "Converted IMG_9771.HEIC → IMG_9771.jpg\n",
            "Converted IMG_9671.HEIC → IMG_9671.jpg\n",
            "Converted IMG_9726.HEIC → IMG_9726.jpg\n",
            "Converted IMG_9717.HEIC → IMG_9717.jpg\n",
            "Converted IMG_9805.HEIC → IMG_9805.jpg\n",
            "Converted IMG_9684.HEIC → IMG_9684.jpg\n",
            "Converted IMG_9730.HEIC → IMG_9730.jpg\n",
            "Converted IMG_9801.HEIC → IMG_9801.jpg\n",
            "Converted IMG_9747.HEIC → IMG_9747.jpg\n",
            "Converted IMG_9710.HEIC → IMG_9710.jpg\n",
            "Converted IMG_9703.HEIC → IMG_9703.jpg\n",
            "Converted IMG_9719.HEIC → IMG_9719.jpg\n",
            "Converted IMG_9729.HEIC → IMG_9729.jpg\n",
            "Converted IMG_9746.HEIC → IMG_9746.jpg\n",
            "Converted IMG_9689.HEIC → IMG_9689.jpg\n",
            "Converted IMG_9668.HEIC → IMG_9668.jpg\n",
            "Converted IMG_9776.HEIC → IMG_9776.jpg\n",
            "Converted IMG_9780.HEIC → IMG_9780.jpg\n",
            "Converted IMG_9799.HEIC → IMG_9799.jpg\n",
            "Converted IMG_9783.HEIC → IMG_9783.jpg\n",
            "Converted IMG_9679.HEIC → IMG_9679.jpg\n",
            "Converted IMG_9736.HEIC → IMG_9736.jpg\n",
            "Converted IMG_9725.HEIC → IMG_9725.jpg\n",
            "Converted IMG_9691.HEIC → IMG_9691.jpg\n",
            "Converted IMG_9696.HEIC → IMG_9696.jpg\n",
            "Converted IMG_9772.HEIC → IMG_9772.jpg\n",
            "Converted IMG_9672.HEIC → IMG_9672.jpg\n",
            "Converted IMG_9760.HEIC → IMG_9760.jpg\n",
            "Converted IMG_9666.HEIC → IMG_9666.jpg\n",
            "Converted IMG_9778.HEIC → IMG_9778.jpg\n",
            "Converted IMG_9713.HEIC → IMG_9713.jpg\n",
            "Converted IMG_9658.HEIC → IMG_9658.jpg\n",
            "Converted IMG_9744.HEIC → IMG_9744.jpg\n",
            "Converted IMG_9724.HEIC → IMG_9724.jpg\n",
            "Converted IMG_9723.HEIC → IMG_9723.jpg\n",
            "Converted IMG_9782.HEIC → IMG_9782.jpg\n",
            "Converted IMG_9775.HEIC → IMG_9775.jpg\n",
            "Converted IMG_9728.HEIC → IMG_9728.jpg\n",
            "Converted IMG_9806.HEIC → IMG_9806.jpg\n",
            "Converted IMG_9756.HEIC → IMG_9756.jpg\n",
            "Converted IMG_9701.HEIC → IMG_9701.jpg\n",
            "Converted IMG_9674.HEIC → IMG_9674.jpg\n",
            "Converted IMG_9700.HEIC → IMG_9700.jpg\n",
            "Converted IMG_9738.HEIC → IMG_9738.jpg\n",
            "Converted IMG_9690.HEIC → IMG_9690.jpg\n",
            "Converted IMG_9750.HEIC → IMG_9750.jpg\n",
            "Converted IMG_9740.HEIC → IMG_9740.jpg\n",
            "Converted IMG_9715.HEIC → IMG_9715.jpg\n",
            "Converted IMG_9774.HEIC → IMG_9774.jpg\n",
            "Converted IMG_9755.HEIC → IMG_9755.jpg\n",
            "Converted IMG_9686.HEIC → IMG_9686.jpg\n",
            "Converted IMG_9705.HEIC → IMG_9705.jpg\n",
            "Converted IMG_9714.HEIC → IMG_9714.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_frames_from_videos(BASE_DIR)"
      ],
      "metadata": {
        "id": "oyH_9pTe58VB"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SiamesePairDataset(USER_DIR, NEG_DIR, train_transform)"
      ],
      "metadata": {
        "id": "qV0yEJzF3et_"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "opQmt7wC38pb"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EmbeddingNet()"
      ],
      "metadata": {
        "id": "Vqm3YZj33-n7"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_siamese(model, loader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yOMznR14HYK",
        "outputId": "0cf1cc41-d2c5-4bcb-81ba-e915d4b2f1ed"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 80/80 [02:03<00:00,  1.55s/it, loss=0.0794]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary: Avg Loss = 0.3364 | Time: 123.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 80/80 [02:03<00:00,  1.55s/it, loss=0.043]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary: Avg Loss = 0.0529 | Time: 123.92s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 80/80 [02:05<00:00,  1.57s/it, loss=0.0207]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary: Avg Loss = 0.0344 | Time: 125.62s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 80/80 [02:03<00:00,  1.55s/it, loss=0.0232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary: Avg Loss = 0.0258 | Time: 123.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 80/80 [02:02<00:00,  1.53s/it, loss=0.0193]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary: Avg Loss = 0.0196 | Time: 122.80s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 80/80 [02:02<00:00,  1.53s/it, loss=0.0115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary: Avg Loss = 0.0157 | Time: 122.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 80/80 [02:02<00:00,  1.53s/it, loss=0.00931]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary: Avg Loss = 0.0116 | Time: 122.10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 80/80 [02:00<00:00,  1.51s/it, loss=0.0072]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary: Avg Loss = 0.0097 | Time: 120.58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 80/80 [02:01<00:00,  1.52s/it, loss=0.00735]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary: Avg Loss = 0.0076 | Time: 121.84s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 80/80 [02:01<00:00,  1.52s/it, loss=0.00691]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary: Avg Loss = 0.0066 | Time: 121.71s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_gallery_embeddings(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRaRmD1v4Ixd",
        "outputId": "6aef4ada-07c1-4b68-c5d8-059bd409ed82"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gallery embeddings saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verify_folder(model, NEG_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn_EOMtjk4du",
        "outputId": "d978062c-b170-457d-b82b-004c73bd1c7f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68_108794d10552b6854b9790aaad0a23bc1f964326caac56a6da24902952bc12ca.jpg: Similarity = 0.5265 → NOT YOUR PET\n",
            "44_6e0bf70e22ce2b1f6bb08805349df9f03c3b8b44c4c9de9ef834aa1378e2b815.jpg: Similarity = 0.5343 → NOT YOUR PET\n",
            "3_cce435f923dbe37307a78f74ee651927536e6ef1c5e2e043c8e0f79638b05e61.jpg: Similarity = 0.8317 → MATCH\n",
            "4_8d8a09ef1b33265b74ac9b36dc8e785e81fb2d1a06999d3b0c7a4e7b84304972.jpg: Similarity = 0.5328 → NOT YOUR PET\n",
            "IMG_9753.jpg: Similarity = 0.8982 → MATCH\n",
            "19_20591e15b9b9b39c5877f9a2346fb7e043e53b5b1959152c64e83d09744a3840.jpg: Similarity = 0.6877 → NOT YOUR PET\n",
            "48_ae7b52fadc8a83dbfb835e0b394bb5c99730364ef4a392d0fd7bb68e0b3280a3.jpg: Similarity = 0.6002 → NOT YOUR PET\n",
            "56_5b12db802f2c4380deb0fdfd9b32d13a224b0857fde1395f4bcbff15ed9ddd3b.jpg: Similarity = 0.5523 → NOT YOUR PET\n",
            "20_134b8786e822e426ddac507d1ea88ef717c18a9f631feea6978edebd47197865.jpg: Similarity = 0.6667 → NOT YOUR PET\n",
            "37_a9f5f104be992e23708f1fa82312a8606062ad8b9611fc2373ab61ad42e2c96b.jpg: Similarity = 0.7284 → NOT YOUR PET\n",
            "52_ecb37a17bd20cee583a8e550a1a521004f4c0cac7f2bf86ae5047b15c0f6fa3c.jpg: Similarity = 0.6525 → NOT YOUR PET\n",
            "35_587573daee6ea207210ecbb86bb57820aaa52dc62012c91b797575b408afbb52.jpg: Similarity = 0.6638 → NOT YOUR PET\n",
            "71_55cbba76469a8febad7d01945a870232fa89c77a1820bf3e1b68093f54b8fdd4.jpg: Similarity = 0.6880 → NOT YOUR PET\n",
            "7_aed0bf2b6a1151f20f7b9ddfdb5255b0f517cd69b1d18ed3ad002367b9099aa4.jpg: Similarity = 0.7801 → MATCH\n",
            "29_254146a4391e10ea42c92410480705aa78316fd316ed8c9f821c7539d09a8011.jpg: Similarity = 0.7240 → NOT YOUR PET\n",
            "75_436fc6fc802bb5c1768432edcc86d7b7409028e51d0d6b7808b6661569badcff.jpg: Similarity = 0.5946 → NOT YOUR PET\n",
            "IMG_9708.jpg: Similarity = 0.6477 → NOT YOUR PET\n",
            "28_33e0295387f1caa5559a05f1107c26a201d42a00cb6e511684f44db1ece710ea.jpg: Similarity = 0.7530 → MATCH\n",
            "79_533e3bc912d1a86ce1974d8a9301396ffb3a4621a22e590762da19a21c47b18f.jpg: Similarity = 0.5557 → NOT YOUR PET\n",
            "22_04ffcc60b797801fb036db532d5e549ff6119b9a3e0af30cfe153475846c3ff8.jpg: Similarity = 0.6193 → NOT YOUR PET\n",
            "73_93fb9680b2b697d9e0cc40609c7e77b9a22a55dc5329dda8a171e5c3508beb7c.jpg: Similarity = 0.6304 → NOT YOUR PET\n",
            "78_2e9759164f4d835b7208d7b875457baab79a443612284a49e1190aed7f2cf8c5.jpg: Similarity = 0.5728 → NOT YOUR PET\n",
            "IMG_9740.jpg: Similarity = 0.8917 → MATCH\n",
            "63_ebbd24bffda5eb01b98e1b6c0dea6884ff5ee90dcfc4f9e612076bfb16727270.jpg: Similarity = 0.7023 → NOT YOUR PET\n",
            "IMG_9675.jpg: Similarity = 0.8737 → MATCH\n",
            "54_bff2928a114a45eddf9244a8c84cd23d1d42f0ae4f30e23fd4f0fbb951c10202.jpg: Similarity = 0.5862 → NOT YOUR PET\n",
            "66_6034b18794e571299de1f26e0c7796bd7ef082ed4cdde906cf1c53d5e5aec839.jpg: Similarity = 0.6705 → NOT YOUR PET\n",
            "40_085078b6875f5555fb70854e9a682c9297d2dfe681fc98742656436eaa6375e1.jpg: Similarity = 0.5049 → NOT YOUR PET\n",
            "58_a4e3eb5dd239f2d32712416121db761b3d0a115ef09eaddc2a3f210bef74ff04.jpg: Similarity = 0.5517 → NOT YOUR PET\n",
            "67_acdfe191f35656dd5a818112717138e16aac98c76556094a0ac47b938f61fd9b.jpg: Similarity = 0.5031 → NOT YOUR PET\n",
            "9_331f2dad6cb36b23fcff308a7ed924e044257626cb091b868e2a3ff0e5570469.jpg: Similarity = 0.8412 → MATCH\n",
            "6_eaf81aeaec347247a98a5092cbe412722bdd7bce3d852ad6ebe86a67266a2cea.jpg: Similarity = 0.4543 → NOT YOUR PET\n",
            "26_77c2653f5b547173c6318b59d2510fbf42d9f939fabfde13c4813566d77b13b9.jpg: Similarity = 0.5807 → NOT YOUR PET\n",
            "70_cd2865a1cfcc87059d577265d0c9a4b2b7e36f509a391ec56e318105f54c7759.jpg: Similarity = 0.5568 → NOT YOUR PET\n",
            "IMG_9682.jpg: Similarity = 0.7508 → MATCH\n",
            "45_ebef897041181341e84da4092ddc4b2821ac12b2d694c38ef4f402f1892d8466.jpg: Similarity = 0.6093 → NOT YOUR PET\n",
            "42_025092f2183fa67a90f3480752326aeb9945d33b5a179f5ff4773d524b9ba5cf.jpg: Similarity = 0.5395 → NOT YOUR PET\n",
            "IMG_9696.jpg: Similarity = 0.8017 → MATCH\n",
            "IMG_9691.jpg: Similarity = 0.7877 → MATCH\n",
            "33_31798a769cbaae2942d83458b372daa071d5d3ca5f3f2e778ebfc3c9fa42bfcd.jpg: Similarity = 0.6473 → NOT YOUR PET\n",
            "12_a874b087b95c94e8f50dbf782da8635b59219055ea2221c068407426611685b4.jpg: Similarity = 0.4473 → NOT YOUR PET\n",
            "8_4d1a71fcf880b37200d325909ee3eeb96ad612a996908213105e3b6a75e1268e.jpg: Similarity = 0.6053 → NOT YOUR PET\n",
            "IMG_9722.jpg: Similarity = 0.8860 → MATCH\n",
            "49_6d7c2fe3dca4bf5e26a75cea3e4074a49426404c46daea16c8685262cf1ccfbd.jpg: Similarity = 0.4553 → NOT YOUR PET\n",
            "15_35cc391fe029ed18446397cd858f46fcadcef9d83f8123c7bd371e0b5b66404f.jpg: Similarity = 0.6597 → NOT YOUR PET\n",
            "IMG_9672.jpg: Similarity = 0.8803 → MATCH\n",
            "24_bc7f557c8210dc1d3e8eab1cb003c1b9e9c79f44c1d87a2c8bc0de67ec255bfc.jpg: Similarity = 0.5562 → NOT YOUR PET\n",
            "25_bf8c07231494e4117c44bffa107b2d2d6bc2b9e107a8110832c0b8ac2ef77216.jpg: Similarity = 0.6192 → NOT YOUR PET\n",
            "32_c65793bf542e55a5bdb7449c5a8cdc5f213816dd930ec2b9d4be7a9d48db6e6b.jpg: Similarity = 0.6351 → NOT YOUR PET\n",
            "IMG_9731.jpg: Similarity = 0.8635 → MATCH\n",
            "IMG_9706.jpg: Similarity = 0.8363 → MATCH\n",
            "1_805213422dad12045b9b4f20193e5766b65616a13282651491edd4212c2fb419.jpg: Similarity = 0.4819 → NOT YOUR PET\n",
            "IMG_9702.jpg: Similarity = 0.8136 → MATCH\n",
            "77_4c9b5d36793714ea4e276d53ed7b3ce0d80816cc6d42a8bc7aa019d94917ddc9.jpg: Similarity = 0.5715 → NOT YOUR PET\n",
            "5_dc80682d35c29729988851d51e24e3c44b484580e7e14d230dd9637105c783e6.jpg: Similarity = 0.6894 → NOT YOUR PET\n",
            "60_674e222825af422275e6f61d56f5c031317af5c53a2831c1552dbcbd97fbc1f2.jpg: Similarity = 0.5637 → NOT YOUR PET\n",
            "23_9ec446609b60cd2a52eb916693710cae7833e7744bf577d9f88e7eedcaec6bb9.jpg: Similarity = 0.6840 → NOT YOUR PET\n",
            "30_0887c9293273db34f005a3d5b96725d4ecd3c1e6a7c828fc7f7f5a2064dae283.jpg: Similarity = 0.5646 → NOT YOUR PET\n",
            "10_bc77ddfa9332a3a3aff064bfa198416baed43dda649e8ae2a5a5fc1e2613bef0.jpg: Similarity = 0.4854 → NOT YOUR PET\n",
            "14_31b1e9a00007bae6d079afc5d7682188315f176552c6c39e1948558803e557c1.jpg: Similarity = 0.7909 → MATCH\n",
            "43_de59c04d652dfbfb67c88bebfd06b94474c5ac9127f8fd0a5b23753fbb23bb41.jpg: Similarity = 0.5697 → NOT YOUR PET\n",
            "IMG_9687.jpg: Similarity = 0.8350 → MATCH\n",
            "36_57541728707f911f50635317e577d7caa97e5c67bf20b9f3eea0906d696a640d.jpg: Similarity = 0.5981 → NOT YOUR PET\n",
            "62_f3c70fe7b90111cb3622a2d7ab62cf38b3dcc8f9ab676041c27cfc6e1fbecda8.jpg: Similarity = 0.7224 → NOT YOUR PET\n",
            "61_9bb265d48acfc5b694195440d7aa39a9493eca1535d5960331c9e32cd0b78ece.jpg: Similarity = 0.6679 → NOT YOUR PET\n",
            "IMG_9735.jpg: Similarity = 0.8969 → MATCH\n",
            "IMG_9745.jpg: Similarity = 0.8918 → MATCH\n",
            "18_d2a9d041ad52bb40f424f4ab2809564f6448a86463261e91e60af16c6dee90d9.jpg: Similarity = 0.6021 → NOT YOUR PET\n",
            "65_66dff6b74616b9cfd5c225e0f02512266bfc53cdb025d9b2b8ddd00ba586453b.jpg: Similarity = 0.5373 → NOT YOUR PET\n",
            "2_d37a3bd28c202420c22cdb9d584597eedd0ad54b35b8c35cee87e36745fc8843.jpeg: Similarity = 0.5856 → NOT YOUR PET\n",
            "16_554ec0ca68227b6c35d9cf3f5f1a5e65f1a98d717d405e9f1cad0210b288b3e7.jpg: Similarity = 0.7959 → MATCH\n",
            "13_d11419e5ee82a2a0a9c8639cca39c6f99412d6fdec77c1d7e199ff31c0f29bf2.jpg: Similarity = 0.6791 → NOT YOUR PET\n",
            "27_95591cac261b47b0bb4984ea89452bb07eb551295346ac2a0c575ad095aa839a.jpg: Similarity = 0.4998 → NOT YOUR PET\n",
            "55_ba105749f31c2ef54f470d70d2d4418fd4ee5be3b6a0949c24c8913a03f70217.jpg: Similarity = 0.6829 → NOT YOUR PET\n",
            "72_04fec94ec38c632fd048243286be377b4bb95115ce203d250f43e56c67b8ba79.jpg: Similarity = 0.5132 → NOT YOUR PET\n",
            "17_8e4238103df42e0cba540b57e7bec4f5e83605d37f3361608c347f8ead306874.jpg: Similarity = 0.6031 → NOT YOUR PET\n",
            "76_40300ff631f033f8fb2cf898ec0f8f28599ad882959899a772a9b21a3d6cf6eb.jpg: Similarity = 0.4849 → NOT YOUR PET\n",
            "IMG_9750.jpg: Similarity = 0.8793 → MATCH\n",
            "39_c6b19301671bbbfcbfdcc75b4270062a7a01b8f518df0b718e3f3716dedda6ab.jpg: Similarity = 0.6459 → NOT YOUR PET\n",
            "50_d97248b3e06608b183f2aae110e59488ece8e31aeb6549ec41cfeac81b5fea2d.jpg: Similarity = 0.5220 → NOT YOUR PET\n",
            "64_73f733e77f06087dd93e8b8b0159c492b0305ffcb9dd06a2b18f0c7e24c4052d.jpg: Similarity = 0.6194 → NOT YOUR PET\n",
            "IMG_9757.jpg: Similarity = 0.9043 → MATCH\n",
            "38_70b590ccc2d3b8868221985d9d37779d838fdc7e57b8fa8a81623d3eec069b21.jpg: Similarity = 0.6690 → NOT YOUR PET\n",
            "31_8bdd15e145a11e0c590bd418d1c31fe740a4b33babe2605aa3612772e1254e85.jpg: Similarity = 0.6667 → NOT YOUR PET\n",
            "34_727f2b0ebd9f7a3e71406806898fdc88032fa2f12678b3b4a08ee08116bfe42f.jpg: Similarity = 0.6116 → NOT YOUR PET\n",
            "46_c340155f77297c0f24bfe9d66a9e2a942b9a6c5b84271c05fd8fa88dceea3f57.jpg: Similarity = 0.5856 → NOT YOUR PET\n",
            "IMG_9726.jpg: Similarity = 0.8616 → MATCH\n",
            "21_fec7048350aa2bb127797fb477a2995a2bc571e8d1e6ca134c370d12298fd73f.jpg: Similarity = 0.6915 → NOT YOUR PET\n",
            "0_682cc94744d9045a277e74eb72f56c676665fa1ca051fde71b69a67e8e22a280.jpg: Similarity = 0.4844 → NOT YOUR PET\n",
            "11_abbd6cfc480306f9aaffb4e5d9ec4487252cc8e7bb38a86ddfae413ee527f30c.jpg: Similarity = 0.6040 → NOT YOUR PET\n",
            "41_0e6cfe320a64fe93784bc6d5c3a1ada6c04add524099b27a95aa73fc91a9a196.jpg: Similarity = 0.5812 → NOT YOUR PET\n",
            "53_413af2c21cfdd844e1be8399d6dc1d0478d0dc168d737538f5496659a4a9b8e1.jpg: Similarity = 0.6525 → NOT YOUR PET\n",
            "74_1305263ef8a84db5e4140cc092bab94ba7baf41c79f82ecbe03f1475a83c2d08.jpg: Similarity = 0.5785 → NOT YOUR PET\n",
            "IMG_9665_frame024.jpg: Similarity = 0.9007 → MATCH\n",
            "59_a149b23d30187ba11d4fe3fc4cd4e7c63a5d89daa728bb1325b09f0a49ef60a9.jpg: Similarity = 0.4397 → NOT YOUR PET\n",
            "69_e1f7bca4a716513025f737d2a3c27484e1910858b04f3da73bc3dea4f370a27f.jpg: Similarity = 0.5212 → NOT YOUR PET\n",
            "47_6d2e6ad02d2a1ddcde6700522426502a9f60c7a6b2d225987101a90763fcd732.jpg: Similarity = 0.6525 → NOT YOUR PET\n",
            "IMG_9718.jpg: Similarity = 0.8627 → MATCH\n",
            "51_278a0ec8d585b0844140936210df0326e1667acc56210e37a9f5b36cd8766975.jpg: Similarity = 0.5050 → NOT YOUR PET\n",
            "IMG_9712.jpg: Similarity = 0.8069 → MATCH\n",
            "57_e8fb26a7334f73a9c79e3230fc4cf44c2545f218076be372328dc604030d51e5.jpg: Similarity = 0.5532 → NOT YOUR PET\n",
            "\n",
            " Total files: 101\n",
            "Matches: 26\n",
            "Not Your Pet: 75\n",
            "Not Your Pet Rate: 74.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKrbCqesk7r_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}